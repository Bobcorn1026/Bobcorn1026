{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bobcorn1026/Bobcorn1026/blob/main/MotorImagery_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 卸載舊版本\n",
        "!pip uninstall tensorflow tensorflow_addons -y\n",
        "\n",
        "# 安裝兼容版本\n",
        "!pip install tensorflow==2.10.0\n",
        "!pip install tensorflow-addons==0.21.0\n",
        "\n",
        "# 測試導入\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "print(\"TensorFlow 版本:\", tf.__version__)\n",
        "print(\"TensorFlow Addons 版本:\", tfa.__version__)"
      ],
      "metadata": {
        "id": "46BVR3512R8K",
        "outputId": "06087472-d16d-44d5-f70d-d90fa7006f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-addons 0.23.0\n",
            "Uninstalling tensorflow-addons-0.23.0:\n",
            "  Successfully uninstalled tensorflow-addons-0.23.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.10.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting tensorflow-addons==0.21.0\n",
            "  Downloading tensorflow_addons-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.21.0) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.21.0) (2.13.3)\n",
            "Downloading tensorflow_addons-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8d1b4664b8b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 測試導入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorFlow 版本:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "fEC0ezNSzEds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "!pip install mne\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential, load_model"
      ],
      "metadata": {
        "id": "jBFMKtGvy0BB",
        "outputId": "2e13dad3-0d24-4971-b25f-7c4fe8f37587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow_addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow_addons) (2.13.3)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.18.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1739d7d4a6b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow_addons'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install mne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "odBkGA-gxw_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d68f27ae-2744-4e0a-9869-be3b52e2fa6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow_addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow_addons) (2.13.3)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5fc45910c55>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow_addons'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install mne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "!pip install mne\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, SpatialDropout1D, SpatialDropout2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten, InputSpec, Layer, Concatenate, AveragePooling2D, MaxPooling2D, Reshape, Permute\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, DepthwiseConv2D, LayerNormalization\n",
        "from tensorflow.keras.layers import TimeDistributed, Lambda, AveragePooling1D, Add, Conv1D, Multiply\n",
        "from tensorflow.keras.constraints import max_norm, unit_norm\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import random\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhHd-DIF2PlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "rzs29ktvzMTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "mk-WcE9npmYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Fs = 256             # Sampling frequency\n",
        "n_channels = 4       # Number of channels\n",
        "Wn = 1               # Sampling window duration\n",
        "n_samples = Wn*Fs    # sampling window length per channel\n",
        "\n",
        "n_ff = [2,4,8,16]    # Number of frequency filters for each inception module of EEG-ITNet\n",
        "n_sf = [1,1,1,1]     # Number of spatial filters in each frequency sub-band of EEG-ITNet\n",
        "batch_size = 32\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "erMv3eUpTFNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture (EEG-ITNet)**"
      ],
      "metadata": {
        "id": "dhPEubAS0hO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Network(Chans, Samples, out_type = 'single'):\n",
        "  out_class = 2\n",
        "  Input_block = Input(shape = (Chans, Samples, 1))\n",
        "  drop_rate = 0.2\n",
        "\n",
        "  block1 = Conv2D(n_ff[0], (1, 16), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_1')(Input_block)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[0], activation = 'linear',\n",
        "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_1')(block1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = Activation('elu')(block1)\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block2 = Conv2D(n_ff[1], (1, 32), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_2')(Input_block)\n",
        "  block2 = BatchNormalization()(block2)\n",
        "  block2 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[1], activation = 'linear',\n",
        "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_2')(block2)\n",
        "  block2 = BatchNormalization()(block2)\n",
        "  block2 = Activation('elu')(block2)\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block3 = Conv2D(n_ff[2], (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_3')(Input_block)\n",
        "  block3 = BatchNormalization()(block3)\n",
        "  block3 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[2], activation = 'linear',\n",
        "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_3')(block3)\n",
        "  block3 = BatchNormalization()(block3)\n",
        "  block3 = Activation('elu')(block3)\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block4 = Conv2D(n_ff[3], (1, 128), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_4')(Input_block)\n",
        "  block4 = BatchNormalization()(block4)\n",
        "  block4 = DepthwiseConv2D((Chans, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[3], activation = 'linear',\n",
        "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_4')(block4)\n",
        "  block4 = BatchNormalization()(block4)\n",
        "  block4 = Activation('elu')(block4)\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block = Concatenate(axis = -1)([block1, block2, block3, block4])\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block = AveragePooling2D((1, 4))(block)\n",
        "  block_in = Dropout(drop_rate)(block)\n",
        "\n",
        "  #================================\n",
        "\n",
        "  paddings = tf.constant([[0,0], [0,0], [3,0], [0,0]])\n",
        "  block = tf.pad(block_in, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block = tf.pad(block, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block_out = Add()([block_in, block])\n",
        "\n",
        "\n",
        "  paddings = tf.constant([[0,0], [0,0], [6,0], [0,0]])\n",
        "  block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block = tf.pad(block, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block_out = Add()([block_out, block])\n",
        "\n",
        "\n",
        "  paddings = tf.constant([[0,0], [0,0], [12,0], [0,0]])\n",
        "  block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block = tf.pad(block, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block_out = Add()([block_out, block])\n",
        "\n",
        "\n",
        "  paddings = tf.constant([[0,0], [0,0], [24,0], [0,0]])\n",
        "  block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block = tf.pad(block, paddings, \"CONSTANT\")\n",
        "  block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  block_out = Add()([block_out, block])\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block = block_out\n",
        "\n",
        "  #================================\n",
        "\n",
        "  block = Conv2D(28, (1,1))(block)\n",
        "  block = BatchNormalization()(block)\n",
        "  block = Activation('elu')(block)\n",
        "  block = AveragePooling2D((1,4), data_format='Channels_last')(block) #'Channels_last' As CPU will be used for inference\n",
        "  block = Dropout(drop_rate)(block)\n",
        "  embedded = Flatten()(block)\n",
        "\n",
        "  out = Dense(out_class, activation = 'softmax', kernel_constraint = max_norm(0.2))(embedded)\n",
        "\n",
        "  return Model(inputs = Input_block, outputs = out)\n"
      ],
      "metadata": {
        "id": "sUQbfQz6cGFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation (Training)\n"
      ],
      "metadata": {
        "id": "Y9sZL8ZKzTN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CRfJ9Sme4QCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load train recording CSV files as Pandas Dataframe with shape (Samples,Channels)**"
      ],
      "metadata": {
        "id": "EjWN3NM1_cP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUMHcdbDyVLe"
      },
      "outputs": [],
      "source": [
        "files1 = glob('drive/MyDrive/MotorImagery/Train/*Left*')                                             #Training data (csv files) for 'Left' Imagery\n",
        "files2 = glob('drive/MyDrive/MotorImagery/Train/*Right*')                                            #Training data (csv files) for 'Right' Imagery\n",
        "\n",
        "data1 = pd.DataFrame()                                                                               #dataframe for 'Left' Imagery\n",
        "data2 = pd.DataFrame()                                                                               #dataframe for 'Right' Imagery\n",
        "\n",
        "for f in files1:\n",
        "    #print(f)\n",
        "    df=pd.read_csv(f)\n",
        "    cols_remove=df.columns.tolist()[:1]\n",
        "    df=df.loc[:, ~df.columns.isin(cols_remove)]\n",
        "    df.columns = df.columns.str.replace('RAW_',\"\",1)\n",
        "    df = df.fillna(df.mean())                                                                        #Replacing null values with mean\n",
        "    data1 = data1.append(df,ignore_index=True)\n",
        "\n",
        "for f in files2:\n",
        "    #print(f)\n",
        "    df=pd.read_csv(f)\n",
        "    cols_remove=df.columns.tolist()[:1]\n",
        "    df=df.loc[:, ~df.columns.isin(cols_remove)]\n",
        "    df.columns = df.columns.str.replace('RAW_',\"\",1)\n",
        "    df = df.fillna(df.mean())\n",
        "    data2 = data2.append(df,ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data1)                                                                                           #check"
      ],
      "metadata": {
        "id": "AZrlLAsnSe6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNE Epochs Function**"
      ],
      "metadata": {
        "id": "rWNIhxYMiZMI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_klZV-Nzlpt"
      },
      "outputs": [],
      "source": [
        "def convertDF2MNE(sub):\n",
        "    info = mne.create_info(list(sub.columns), ch_types=['eeg'] * len(sub.columns), sfreq=256)\n",
        "    info.set_montage('standard_1020')\n",
        "    data=mne.io.RawArray(sub.T, info)\n",
        "    data.set_eeg_reference()\n",
        "    epochs=mne.make_fixed_length_epochs(data,duration=Wn,overlap=0.2*Wn)\n",
        "    return epochs.get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to MNE epochs, load as numpy arrays and create labels**"
      ],
      "metadata": {
        "id": "62EOVWIFxRm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_left = np.empty((0, n_channels, n_samples))\n",
        "y_left = np.empty(0)\n",
        "x_right = np.empty((0, n_channels, n_samples))\n",
        "y_right = np.empty(0)\n",
        "\n",
        "data = convertDF2MNE(data1)\n",
        "for i in enumerate(data):\n",
        "  label=0\n",
        "  y_left=np.append(y_left,label)\n",
        "x_left = np.append(x_left,data,axis=0)\n",
        "\n",
        "data = convertDF2MNE(data2)\n",
        "for i in enumerate(data):\n",
        "  label=1\n",
        "  y_right=np.append(y_right,label)\n",
        "x_right = np.append(x_right,data,axis=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g5elMbypsPuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train, Validation Split**"
      ],
      "metadata": {
        "id": "UnWEuuMAiPHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_left = x_left[:,:,:,np.newaxis]\n",
        "x_right = x_right[:,:,:,np.newaxis]\n",
        "split = 64\n",
        "\n",
        "x_train = np.concatenate((x_left[split:,:,:,:],x_right[split:,:,:,:]), axis=0)\n",
        "x_val = np.concatenate((x_left[:split,:,:,:],x_right[:split,:,:,:]), axis=0)\n",
        "\n",
        "y_train = np.concatenate((y_left[split:],y_right[split:]))\n",
        "y_val = np.concatenate((y_left[:split],y_right[:split]))"
      ],
      "metadata": {
        "id": "vkPPFF-uZeFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for any 'NaN' values**"
      ],
      "metadata": {
        "id": "vuSMiK6VyHt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.argwhere(np.isnan(x_train))\n"
      ],
      "metadata": {
        "id": "VZe1U9fiA0m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Test Data"
      ],
      "metadata": {
        "id": "G7OVrt3OAccq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load test recording CSV files as pandas dataframe**"
      ],
      "metadata": {
        "id": "tPQm1nEdwKsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files1 = glob('drive/MyDrive/MotorImagery/Test/*Left*')                                                  #Test data for 'Left' Imagery\n",
        "files2 = glob('drive/MyDrive/MotorImagery/Test/*Right*')                                                 #Test data for 'Right' Imagery\n",
        "\n",
        "data1_test = pd.DataFrame()\n",
        "data2_test = pd.DataFrame()\n",
        "\n",
        "for f in files1:\n",
        "    #print(f)\n",
        "    df=pd.read_csv(f)\n",
        "    cols_remove=df.columns.tolist()[:1]\n",
        "    df=df.loc[:, ~df.columns.isin(cols_remove)]\n",
        "    df.columns = df.columns.str.replace('RAW_',\"\",1)\n",
        "    df = df.fillna(df.mean())\n",
        "    data1_test = data1_test.append(df,ignore_index=True)\n",
        "\n",
        "for f in files2:\n",
        "    #print(f)\n",
        "    df=pd.read_csv(f)\n",
        "    cols_remove=df.columns.tolist()[:1]\n",
        "    df=df.loc[:, ~df.columns.isin(cols_remove)]\n",
        "    df.columns = df.columns.str.replace('RAW_',\"\",1)\n",
        "    df = df.fillna(df.mean())\n",
        "    data2_test = data2_test.append(df,ignore_index=True)"
      ],
      "metadata": {
        "id": "PtnP4abA_rX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to MNE Epochs and load as numpy arrays with labels**"
      ],
      "metadata": {
        "id": "sbtR0xTXwRQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.empty((0, n_channels, n_samples))\n",
        "y_test = np.empty(0)\n",
        "\n",
        "data = convertDF2MNE(data1_test)\n",
        "for i in enumerate(data):\n",
        "  label=0\n",
        "  y_test=np.append(y_test,label)\n",
        "x_test = np.append(x_test,data,axis=0)\n",
        "\n",
        "data = convertDF2MNE(data2_test)\n",
        "for i in enumerate(data):\n",
        "  label=1\n",
        "  y_test=np.append(y_test,label)\n",
        "x_test = np.append(x_test,data,axis=0)\n",
        "\n",
        "x_test = x_test[:,:,:,np.newaxis]"
      ],
      "metadata": {
        "id": "sCt4Esp4Anec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization (optional)"
      ],
      "metadata": {
        "id": "M0QaqpwRyHoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to MNE Epochs**"
      ],
      "metadata": {
        "id": "dnAEoHguLhin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = convertDF2MNE(data1)\n",
        "test2 = convertDF2MNE(data2)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "6H-igjRd18_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left,Right Comparison - TP9,TP10**"
      ],
      "metadata": {
        "id": "T_82avb5KBgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0, len(test1[0][0]))\n",
        "y1 = test1[0][0]\n",
        "y2 = test2[1][0]\n",
        "y3 = test1[1][3]\n",
        "y4 = test2[1][3]\n",
        "\n",
        "figure, axis = plt.subplots(2, 2)\n",
        "\n",
        "# For Channel 1\n",
        "axis[0, 0].plot(x, y1)\n",
        "axis[0, 0].set_title(\"Left_TP9\")\n",
        "\n",
        "# For Channel 2\n",
        "axis[0, 1].plot(x, y2)\n",
        "axis[0, 1].set_title(\"Right_TP9\")\n",
        "\n",
        "# For Channel 1\n",
        "axis[1, 0].plot(x, y3)\n",
        "axis[1, 0].set_title(\"Left_TP10\")\n",
        "\n",
        "# For Channel 2\n",
        "axis[1, 1].plot(x, y4)\n",
        "axis[1, 1].set_title(\"Right_TP10\")\n",
        "\n",
        "# Combine all the operations and display\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XbT9HJigaTPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left,Right Comparison - AF7,AF8**"
      ],
      "metadata": {
        "id": "fM8zrgidMxeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0, len(test1[1][0]))\n",
        "y1 = test1[0][1]\n",
        "y2 = test2[0][1]\n",
        "y3 = test1[0][2]\n",
        "y4 = test2[0][2]\n",
        "\n",
        "figure, axis = plt.subplots(2, 2)\n",
        "\n",
        "# For Channel 1\n",
        "axis[0, 0].plot(x, y1)\n",
        "axis[0, 0].set_title(\"Left_AF7\")\n",
        "\n",
        "# For Channel 2\n",
        "axis[0, 1].plot(x, y2)\n",
        "axis[0, 1].set_title(\"Right_AF7\")\n",
        "\n",
        "# For Channel 1\n",
        "axis[1, 0].plot(x, y3)\n",
        "axis[1, 0].set_title(\"Left_AF8\")\n",
        "\n",
        "# For Channel 2\n",
        "axis[1, 1].plot(x, y4)\n",
        "axis[1, 1].set_title(\"Right_AF8\")\n",
        "\n",
        "# Combine all the operations and display\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_nS62VWSdcj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Data Visualization**"
      ],
      "metadata": {
        "id": "Mm-Sv_CHMMZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "B2Yd8drjwgoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data to be plotted\n",
        "x = np.arange(0, len(x_train[1][0]))\n",
        "y1 = x_train[2][0]\n",
        "y2 = x_train[2][1]\n",
        "y3 = x_train[2][2]\n",
        "y4 = x_train[2][3]\n",
        "\n",
        "figure, axis = plt.subplots(2, 2)\n",
        "\n",
        "# For Channel 1\n",
        "axis[0, 0].plot(x, y1)\n",
        "axis[0, 0].set_title(\"TP9\")\n",
        "\n",
        "# For Channel 2\n",
        "axis[0, 1].plot(x, y2)\n",
        "axis[0, 1].set_title(\"AF7\")\n",
        "\n",
        "# For Channel 3\n",
        "axis[1, 0].plot(x, y3)\n",
        "axis[1, 0].set_title(\"AF8\")\n",
        "\n",
        "# For Channel 4\n",
        "axis[1, 1].plot(x, y4)\n",
        "axis[1, 1].set_title(\"TP10\")\n",
        "\n",
        "# Combine all the operations and display\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdLzxQ-_vWPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "jtu7xKkIyWNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data loader function**"
      ],
      "metadata": {
        "id": "sir0BIWmskny"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4x1Zh1DGmOR"
      },
      "source": [
        "class DataLoader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, images, labels, batch_size=32, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.key_array = np.arange(self.images.shape[0], dtype=np.uint32)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.key_array)//self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        keys = self.key_array[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x = np.asarray(self.images[keys], dtype=np.float32)\n",
        "        y = np.asarray(self.labels[keys], dtype=np.float32)\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.key_array = np.random.permutation(self.key_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load training data**"
      ],
      "metadata": {
        "id": "wmSLA-Aqs3DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(images=x_train, labels=y_train, batch_size=32, shuffle=True)\n",
        "n_batches = len(dataloader)"
      ],
      "metadata": {
        "id": "d7lY7K1V9yq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Optimizer, loss functions**"
      ],
      "metadata": {
        "id": "lreHTMYjs8Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "loss_train = np.zeros(shape=(epochs,), dtype=np.float32)\n",
        "acc_train = np.zeros(shape=(epochs,), dtype=np.float32)\n",
        "loss_val = np.zeros(shape=(epochs,))\n",
        "acc_val = np.zeros(shape=(epochs,))"
      ],
      "metadata": {
        "id": "4Sm3etpP-W-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load model**"
      ],
      "metadata": {
        "id": "isyOHPrRswQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network(Chans=n_channels,Samples=n_samples)\n",
        "model.compile(optimizer=optimizer,loss=ce_loss)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2w_1IRG--4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Epochs**"
      ],
      "metadata": {
        "id": "jFoY0m1njWap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean() # Keeping track of the training loss\n",
        "  epoch_acc_avg = tf.keras.metrics.Mean() # Keeping track of the training accuracy\n",
        "\n",
        "  print('==== Epoch #{0:3d} ===='.format(epoch))\n",
        "\n",
        "  for batch in tqdm(range(n_batches)):\n",
        "    x, y = dataloader[batch]\n",
        "\n",
        "    with tf.GradientTape() as tape: # Forward pass\n",
        "      y_ = model(x, training=True)\n",
        "      loss = ce_loss(y_true=y, y_pred=y_)\n",
        "\n",
        "    grad = tape.gradient(loss, model.trainable_variables) # Backpropagation\n",
        "    optimizer.apply_gradients(zip(grad, model.trainable_variables)) # Update network weights\n",
        "\n",
        "    epoch_loss_avg(loss)\n",
        "    epoch_acc_avg(sklearn.metrics.accuracy_score(y_true=y, y_pred=np.argmax(y_, axis=-1)))\n",
        "\n",
        "  dataloader.on_epoch_end()\n",
        "\n",
        "  loss_train[epoch] = epoch_loss_avg.result()\n",
        "  acc_train[epoch] = epoch_acc_avg.result()\n",
        "\n",
        "  print('---- Training ----')\n",
        "  print('Loss  =  {0:.3f}'.format(loss_train[epoch]))\n",
        "  print('Acc   =  {0:.3f}'.format(acc_train[epoch]))\n",
        "\n",
        "  y_ = model.predict(x_val) # Validation predictions\n",
        "  loss_val[epoch] = ce_loss(y_true=y_val, y_pred=y_).numpy()\n",
        "  acc_val[epoch] = sklearn.metrics.accuracy_score(y_true=y_val, y_pred=np.argmax(y_, axis=-1))\n",
        "\n",
        "  print('--- Validation ---')\n",
        "  print('Loss  =  {0:.3f}'.format(loss_val[epoch]))\n",
        "  print('Acc   =  {0:.3f}'.format(acc_val[epoch]))"
      ],
      "metadata": {
        "id": "276sTUjW_7Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save model**"
      ],
      "metadata": {
        "id": "PFkAIb3lt835"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/MotorImagery/Train_model/model.h5\")"
      ],
      "metadata": {
        "id": "F7TtTdaUIjQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "WYQQSk_lyqdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load model**"
      ],
      "metadata": {
        "id": "s-p67pbNuFYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = tf.keras.models.load_model('/content/drive/MyDrive/MotorImagery/Train_model/model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "rg_ymmAfIt3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test model**"
      ],
      "metadata": {
        "id": "SOP6V-jDusHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_ = test_model.predict(x_test)                                                                      # Test predictions\n",
        "\n",
        "loss_val[0] = ce_loss(y_true=y_test, y_pred=y_).numpy()\n",
        "acc_val[0] = sklearn.metrics.accuracy_score(y_true=y_test, y_pred=np.argmax(y_, axis=-1))\n",
        "\n",
        "for i in range(x_test.shape[0]):\n",
        "  if y_[i][0]>y_[i][1]:\n",
        "    print('Predicted value : 0 with accuracy = {0:.3f}'.format(y_[i][0]))\n",
        "  else:\n",
        "    print('Predicted value : 1 with accuracy = {0:.3f}'.format(y_[i][1]))\n",
        "  print('Actual value = {}\\n'.format(y_test[i]))\n",
        "\n",
        "print('--- Test ---')\n",
        "print('Loss  =  {0:.3f}'.format(loss_val[0]))\n",
        "print('Acc   =  {0:.3f}'.format(acc_val[0]))"
      ],
      "metadata": {
        "id": "jvUVvurWE-88"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}